{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "path_dataset = \"Project2Data.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))\n",
    "#displaying of data for items needs fixing, probably d too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1000\n",
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (10000, 1000)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:1065327\n",
      "Total number of nonzero elements in test data:111625\n"
     ]
    }
   ],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]  \n",
    "    \n",
    "    # init\n",
    "    num_rows, num_cols = valid_ratings.shape\n",
    "    print(num_rows)\n",
    "    print(num_cols)\n",
    "    train = sp.lil_matrix((num_rows, num_cols))\n",
    "    test = sp.lil_matrix((num_rows, num_cols))\n",
    "    \n",
    "    print(\"the shape of original ratings. (# of row, # of col): {}\".format(\n",
    "        ratings.shape))\n",
    "    print(\"the shape of valid ratings. (# of row, # of col): {}\".format(\n",
    "        (num_rows, num_cols)))\n",
    "    \n",
    "    nz_items, nz_users = valid_ratings.nonzero()\n",
    "    \n",
    "    # split data\n",
    "    for user in set(nz_users):\n",
    "        # randomly select a subset of ratings\n",
    "        row, col = valid_ratings[:, user].nonzero()\n",
    "        selects = np.random.choice(row, size=int(len(row) * p_test))\n",
    "        residual = list(set(row) - set(selects))\n",
    "\n",
    "        # add to train set\n",
    "        train[residual, user] = valid_ratings[residual, user]\n",
    "\n",
    "        # add to test set\n",
    "        test[selects, user] = valid_ratings[selects, user]\n",
    "    \n",
    "    \n",
    "    print(\"Total number of nonzero elements in origial data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    return valid_ratings, train, test\n",
    "valid_ratings, train, test = split_data(ratings, num_items_per_user, num_users_per_item, 0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    num_item, num_user = train.get_shape()\n",
    "\n",
    "    user_features = np.random.rand(num_features, num_user)\n",
    "    item_features = np.random.rand(num_features, num_item)\n",
    "\n",
    "    # start by item features.\n",
    "    item_nnz = train.getnnz(axis=1)\n",
    "    item_sum = train.sum(axis=1)\n",
    "\n",
    "    for ind in range(num_item):\n",
    "        item_features[0, ind] = item_sum[ind, 0] / item_nnz[ind]\n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10000)\n",
      "(20, 1000)\n",
      "(10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "user_features, item_features = init_MF(train, 20)\n",
    "print(item_features.shape)\n",
    "print(user_features.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    mse = 0\n",
    "    for row, col in nz:\n",
    "        item_info = item_features[:, row]\n",
    "        user_info = user_features[:, col]\n",
    "        mse += (data[row, col] - user_info.T.dot(item_info)) ** 2\n",
    "    return np.sqrt(1.0 * mse / len(nz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_SGD(train, test, lambda_user, lambda_item, gamma, K, iterMax):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    gamma = gamma\n",
    "    num_features = K   # K in the lecture notes\n",
    "    lambda_user = lambda_user\n",
    "    lambda_item = lambda_item\n",
    "    num_epochs = iterMax     # number of full passes through the train set\n",
    "    errors = [0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "        \n",
    "        for d, n in nz_train:\n",
    "            # update W_d (item_features[:, d]) and Z_n (user_features[:, n])\n",
    "            item_info = item_features[:, d]\n",
    "            user_info = user_features[:, n]\n",
    "            err = train[d, n] - user_info.T.dot(item_info)\n",
    "    \n",
    "            # calculate the gradient and update\n",
    "            item_features[:, d] += gamma * (err * user_info - lambda_item * item_info)\n",
    "            user_features[:, n] += gamma * (err * item_info - lambda_user * user_info)\n",
    "\n",
    "        rmse = compute_error(train, user_features, item_features, nz_train)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "        errors.append(rmse)\n",
    "\n",
    "    # evaluate the test error\n",
    "    rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_SGD_all(train, test, lambda_user, lambda_item, gamma, K, iterMax):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    gamma = gamma\n",
    "    num_features = K   # K in the lecture notes\n",
    "    lambda_user = lambda_user\n",
    "    lambda_item = lambda_item\n",
    "    num_epochs = iterMax     # number of full passes through the train set\n",
    "    errors = [0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "        \n",
    "        for d, n in nz_train:\n",
    "            # update W_d (item_features[:, d]) and Z_n (user_features[:, n])\n",
    "            item_info = item_features[:, d]\n",
    "            user_info = user_features[:, n]\n",
    "            err = train[d, n] - user_info.T.dot(item_info)\n",
    "    \n",
    "            # calculate the gradient and update\n",
    "            item_features[:, d] += gamma * (err * user_info - lambda_item * item_info)\n",
    "            user_features[:, n] += gamma * (err * item_info - lambda_user * user_info)\n",
    "\n",
    "        rmse = compute_error(train, user_features, item_features, nz_train)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "        errors.append(rmse)\n",
    "\n",
    "    # evaluate the test error\n",
    "    rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    return rmse, user_features, item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 1:\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.049632360853516.\n",
      "iter: 1, RMSE on training set: 1.020243267982725.\n",
      "iter: 2, RMSE on training set: 1.0002199574032948.\n",
      "iter: 3, RMSE on training set: 0.9862894450076296.\n",
      "iter: 4, RMSE on training set: 0.9729548735154872.\n",
      "iter: 5, RMSE on training set: 0.9609459920159176.\n",
      "iter: 6, RMSE on training set: 0.9524951700296076.\n",
      "iter: 7, RMSE on training set: 0.9447869899052078.\n",
      "iter: 8, RMSE on training set: 0.9386746993758812.\n",
      "iter: 9, RMSE on training set: 0.9339996050808759.\n",
      "RMSE on test data: 1.0123675593615962.\n",
      "ROUND 2:\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0397563719029288.\n",
      "iter: 1, RMSE on training set: 1.0161499347899725.\n",
      "iter: 2, RMSE on training set: 1.0016438189094534.\n",
      "iter: 3, RMSE on training set: 0.9901798424824607.\n",
      "iter: 4, RMSE on training set: 0.9775786548229987.\n",
      "iter: 5, RMSE on training set: 0.9659991072591453.\n",
      "iter: 6, RMSE on training set: 0.9567974086356138.\n",
      "iter: 7, RMSE on training set: 0.9487041952091869.\n",
      "iter: 8, RMSE on training set: 0.94218980915821.\n",
      "iter: 9, RMSE on training set: 0.936939469040302.\n",
      "RMSE on test data: 1.0104964617612586.\n",
      "ROUND 3:\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0445929291860059.\n",
      "iter: 1, RMSE on training set: 1.0468781342203075.\n",
      "iter: 2, RMSE on training set: 1.0418405378700002.\n",
      "iter: 3, RMSE on training set: 1.0318581032830494.\n",
      "iter: 4, RMSE on training set: 1.0169128023446647.\n",
      "iter: 5, RMSE on training set: 1.0010587076661135.\n",
      "iter: 6, RMSE on training set: 0.9890323459789432.\n",
      "iter: 7, RMSE on training set: 0.9781295418132661.\n",
      "iter: 8, RMSE on training set: 0.9703196381724453.\n",
      "iter: 9, RMSE on training set: 0.9630365399108027.\n",
      "RMSE on test data: 1.0169757415503682.\n",
      "ROUND 1:\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.054463904479123.\n",
      "iter: 1, RMSE on training set: 1.0279302666284864.\n",
      "iter: 2, RMSE on training set: 1.01153704483075.\n",
      "iter: 3, RMSE on training set: 1.0009576138284326.\n",
      "iter: 4, RMSE on training set: 0.9931169488106398.\n",
      "iter: 5, RMSE on training set: 0.9868342670673739.\n",
      "iter: 6, RMSE on training set: 0.9830207150804375.\n",
      "iter: 7, RMSE on training set: 0.9792039513028759.\n",
      "iter: 8, RMSE on training set: 0.976611421882097.\n",
      "iter: 9, RMSE on training set: 0.9742294631380961.\n",
      "RMSE on test data: 1.0050841804191528.\n",
      "ROUND 2:\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0503324714001314.\n",
      "iter: 1, RMSE on training set: 1.01957995583671.\n",
      "iter: 2, RMSE on training set: 1.0076287612566877.\n",
      "iter: 3, RMSE on training set: 1.0014013265550359.\n",
      "iter: 4, RMSE on training set: 0.997010808601072.\n",
      "iter: 5, RMSE on training set: 0.9937410168456107.\n",
      "iter: 6, RMSE on training set: 0.9910108854772394.\n",
      "iter: 7, RMSE on training set: 0.9887430398337889.\n",
      "iter: 8, RMSE on training set: 0.9867624584516931.\n",
      "iter: 9, RMSE on training set: 0.9849628193117621.\n",
      "RMSE on test data: 1.0062777067972886.\n",
      "ROUND 3:\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0450649477841845.\n",
      "iter: 1, RMSE on training set: 1.032524144010261.\n",
      "iter: 2, RMSE on training set: 1.028147305222596.\n",
      "iter: 3, RMSE on training set: 1.0247182031517874.\n",
      "iter: 4, RMSE on training set: 1.0201436246803355.\n",
      "iter: 5, RMSE on training set: 1.0161979335179825.\n",
      "iter: 6, RMSE on training set: 1.0126041258572318.\n",
      "iter: 7, RMSE on training set: 1.009875765033771.\n",
      "iter: 8, RMSE on training set: 1.007548748076564.\n",
      "iter: 9, RMSE on training set: 1.0052705045343506.\n",
      "RMSE on test data: 1.0179207562764456.\n",
      "ROUND 1:\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.057096330303997.\n",
      "iter: 1, RMSE on training set: 1.034476827736688.\n",
      "iter: 2, RMSE on training set: 1.019832310037675.\n",
      "iter: 3, RMSE on training set: 1.0100657806383668.\n",
      "iter: 4, RMSE on training set: 1.0033793241511682.\n",
      "iter: 5, RMSE on training set: 0.9983612394758729.\n",
      "iter: 6, RMSE on training set: 0.9956568809653619.\n",
      "iter: 7, RMSE on training set: 0.9930049006085364.\n",
      "iter: 8, RMSE on training set: 0.9916172673812131.\n",
      "iter: 9, RMSE on training set: 0.9898558895982246.\n",
      "RMSE on test data: 1.009017855077843.\n",
      "ROUND 2:\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.061689472290075.\n",
      "iter: 1, RMSE on training set: 1.0305411020941178.\n",
      "iter: 2, RMSE on training set: 1.0166082023290803.\n",
      "iter: 3, RMSE on training set: 1.0100048344788695.\n",
      "iter: 4, RMSE on training set: 1.006491194354878.\n",
      "iter: 5, RMSE on training set: 1.0044294246134746.\n",
      "iter: 6, RMSE on training set: 1.002499552722739.\n",
      "iter: 7, RMSE on training set: 1.0012307796338007.\n",
      "iter: 8, RMSE on training set: 1.000169716097563.\n",
      "iter: 9, RMSE on training set: 0.9988778607370208.\n",
      "RMSE on test data: 1.0130179057222053.\n",
      "ROUND 3:\n",
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0626199138490953.\n",
      "iter: 1, RMSE on training set: 1.0388021618959713.\n",
      "iter: 2, RMSE on training set: 1.0341499338562405.\n",
      "iter: 3, RMSE on training set: 1.0313250103746658.\n",
      "iter: 4, RMSE on training set: 1.0282825656065393.\n",
      "iter: 5, RMSE on training set: 1.0258529559750118.\n",
      "iter: 6, RMSE on training set: 1.0229315249622017.\n",
      "iter: 7, RMSE on training set: 1.021253905319032.\n",
      "iter: 8, RMSE on training set: 1.019626596595818.\n",
      "iter: 9, RMSE on training set: 1.0177782183219397.\n",
      "RMSE on test data: 1.028553360727704.\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.02  #0.03 best for K = 20 and initial\n",
    "rmse_ev = np.zeros([3,3])\n",
    "lambda_user = [0.01, 0.05, 0.1]  #best for K = 20 and gamma = 0.03 initial\n",
    "lambda_item = [0.1, 0.2, 0.5]  #best for K = 20 lu = 0.02 and gamma = 0.03\n",
    "num_features = 15   # K in the lecture notes\n",
    "num_epochs = 10\n",
    "i = 0\n",
    "for lu in lambda_user:\n",
    "    i += 1\n",
    "    j = 0\n",
    "    for li in lambda_item:\n",
    "        j += 1\n",
    "        print(\"ROUND {},{}:\".format(i,j))\n",
    "        rmse_ev[i-1, j-1] = matrix_factorization_SGD(train, test, lu, li, gamma, num_features, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00338357 1.00326083 1.00456928]\n",
      " [1.00341493 1.00323543 1.00462914]\n",
      " [1.00405953 1.00393648 1.00545454]]\n"
     ]
    }
   ],
   "source": [
    "print(rmse_ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave the rest of the code above as it is and only work with the stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.050126919981355.\n",
      "iter: 1, RMSE on training set: 1.0225215943770478.\n",
      "iter: 2, RMSE on training set: 1.0076296712272828.\n",
      "iter: 3, RMSE on training set: 1.0008576142759364.\n",
      "iter: 4, RMSE on training set: 0.9961661440532399.\n",
      "iter: 5, RMSE on training set: 0.9914289947696661.\n",
      "iter: 6, RMSE on training set: 0.9888327994671374.\n",
      "iter: 7, RMSE on training set: 0.9862181778032121.\n",
      "iter: 8, RMSE on training set: 0.9836731263175907.\n",
      "iter: 9, RMSE on training set: 0.9820409403228237.\n",
      "iter: 10, RMSE on training set: 0.9806163551286149.\n",
      "iter: 11, RMSE on training set: 0.9796549112360273.\n",
      "iter: 12, RMSE on training set: 0.9786522233614954.\n",
      "iter: 13, RMSE on training set: 0.9777877813307858.\n",
      "iter: 14, RMSE on training set: 0.9773366942458743.\n",
      "iter: 15, RMSE on training set: 0.9767997375131514.\n",
      "iter: 16, RMSE on training set: 0.9763927293071198.\n",
      "iter: 17, RMSE on training set: 0.9761353815559234.\n",
      "iter: 18, RMSE on training set: 0.9759279692792843.\n",
      "iter: 19, RMSE on training set: 0.9757921291433297.\n",
      "RMSE on test data: 0.9989210993421344.\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.02  #0.03 best for K = 20 and initial\n",
    "lambda_user = 0.03 #best for K = 20 and gamma = 0.03 initial\n",
    "lambda_item = 0.15  #best for K = 20 lu = 0.02 and gamma = 0.03\n",
    "num_features = 5# K in the lecture notes\n",
    "num_epochs = 20 \n",
    "\n",
    "rmse_ev, Z, W  = matrix_factorization_SGD_all(train, test, lambda_user, lambda_item, gamma, num_features, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_lines(line):\n",
    "    row_cols = line[0]\n",
    "    r, c = row_cols.split(\"_\")\n",
    "    row = r[1:]\n",
    "    col = c[1:]\n",
    "    return int(row), int(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_Sample(path):\n",
    "    import csv\n",
    "    with open(path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        rows = list()\n",
    "        cols = list()\n",
    "        for row in csv_reader:\n",
    "            csv_reader = [deal_lines(line) for line in csv_reader]\n",
    "    return csv_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176952\n"
     ]
    }
   ],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "sampleSub = \"SampleSubmission.csv\"\n",
    "data = read_Sample(sampleSub)\n",
    "indices = np.asarray(data);\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcValue(rows, cols):\n",
    "    value = int(round(Z[:,cols].dot(W[:,rows].T)))\n",
    "    a = [1, 2, 3, 4, 5]\n",
    "    if value not in a:\n",
    "        if value > 5:\n",
    "            value = 5\n",
    "        if value < 1:\n",
    "            value = 1\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176952\n"
     ]
    }
   ],
   "source": [
    "def writeSubFile(indices):\n",
    "    import csv\n",
    "    with open('SubmissionFile.csv', mode='w') as submF:\n",
    "        writer = csv.writer(submF, delimiter=',', quotechar='', quoting=csv.QUOTE_NONE , lineterminator = '\\n')\n",
    "        writer.writerow(['Id','Prediction'])\n",
    "        tot = 0\n",
    "        for row, col in indices:\n",
    "            msg = 'r'+str(row)+'_c'+str(col)\n",
    "            value = calcValue(row-1, col-1)\n",
    "            writer.writerow([msg, value])\n",
    "            tot += 1\n",
    "        print(tot)\n",
    "writeSubFile(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
